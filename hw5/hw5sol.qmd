---
title: "Biostat 203B Homework 5"
subtitle: Due Mar 22 @ 11:59PM
author: "Yang An and UID: 106332601"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
---

## Predicting ICU duration

Using the ICU cohort `mimiciv_icu_cohort.rds` you built in Homework 4, develop at least three machine learning approaches (logistic regression with enet regularization, random forest, boosting, SVM, MLP, etc) plus a model stacking approach for predicting whether a patient's ICU stay will be longer than 2 days. You should use the `los_long` variable as the outcome. You algorithms can use patient demographic information (gender, age at ICU `intime`, marital status, race), ICU admission information (first care unit), the last lab measurements before the ICU stay, and first vital measurements during ICU stay as features. You are welcome to use any feature engineering techniques you think are appropriate; but make sure to not use features that are not available at an ICU stay's `intime`. For instance, `last_careunit` cannot be used in your algorithms. 

1. Data preprocessing and feature engineering.

2. Partition data into 50% training set and 50% test set. Stratify partitioning according to `los_long`. For grading purpose, sort the data by `subject_id`, `hadm_id`, and `stay_id` and use the seed `203` for the initial data split. 

```{r}
rm(list = ls())
library(tidymodels)
library(dplyr)
library(bigrquery)
library(dbplyr)
library(DBI)
library(gt)
library(gtsummary)
library(tidyverse)
set.seed(203)
# Load data
mimiciv_icu_cohort <- readRDS("mimiciv_shiny/mimic_icu_cohort.rds")

# sort
mimiciv_icu_cohort <- mimiciv_icu_cohort |>
  arrange(subject_id, hadm_id, stay_id) |>
  select(-admit_provider_id, -last_careunit, -deathtime, -dischtime, -edouttime,-edregtime, -discharge_location
         ) |>
  mutate(hospital_expire_flag = as.factor(hospital_expire_flag)) |>
  mutate(dod = as.POSIXct(dod)) |>
  mutate(los_long = los > 2) |>
  mutate(age = anchor_age + year(intime) - anchor_year) |>
  mutate(age = ifelse(age < 0, NA, age)) |>
  mutate(age = as.numeric(age)) |>
  mutate(los_long = as.factor(los_long)) |>
  mutate(race = as.factor(race)) |>
  mutate(marital_status = as.factor(marital_status)) |>
  mutate(gender = as.factor(gender)) |>
  mutate(langeuage = as.factor(language)) |>
  mutate(insurance = as.factor(insurance)) |>
  mutate(admission_type = as.factor(admission_type)) |>
  mutate(admission_location = as.factor(admission_location)) |>
  mutate(age = as.numeric(age)) |>
  select(-los, -anchor_age, -anchor_year, -anchor_year_group, -outtime, -intime, -admittime, -dod) |>
  print() 
glimpse(mimiciv_icu_cohort)
```

```{r}
mimiciv_icu_cohort |> 
  map_df(~sum(is.na(.x))) |>
  gather(variable, na_count) |>
  filter(na_count > 0) |>
  arrange(desc(na_count)) |>
  gt()
```


```{r}
data_split <- initial_split(
  mimiciv_icu_cohort, 
  # stratify by los_long
  strata = "los_long", 
  prop = 0.5
  )
```


```{r}
library(gtsummary)
# Numerical summaries stratified by the outcome `los_long`.
mimiciv_icu_cohort |> tbl_summary(by = los_long)
```


3. Train and tune the models using the training set.

```{r}
mimic_other <- training(data_split)
dim(mimic_other)
```
```{r}
mimic_test <- testing(data_split)
dim(mimic_test)
```



###logit

```{r}
logit_recipe <- 
  recipe(
    los_long ~ .,
    data = mimic_other
  ) |>
  # mean imputation for 
  step_impute_mean(all_numeric_predictors()) |>
  # mode imputation for 
  step_impute_mode(all_nominal_predictors()) |>
  # create traditional dummy variables (necessary for svm)
  step_dummy(all_nominal_predictors()) |>
  # zero-variance filter
  step_zv(all_numeric_predictors()) |> 
  # center and scale numeric data
  step_normalize(all_numeric_predictors()) 

```
```{r}
prepped_recipe <- prep(logit_recipe)
baked_data <- bake(prepped_recipe, new_data = NULL) 
baked_data
```


```{r}
logit_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = tune()
  ) |> 
  set_engine("glmnet", standardize = FALSE) |>
  print()
```

```{r}
logit_wf <- workflow() |>
  add_recipe(logit_recipe) |>
  add_model(logit_mod) |>
  print()
```

```{r}
param_grid <- grid_regular(
  penalty(range = c(-3, 1)), 
  mixture(),
  levels = c(5, 3)
)

```

```{r}
set.seed(203)

folds <- vfold_cv(mimic_other, v = 5)
folds
```

```{r}
(logit_fit <- logit_wf |>
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )) |>
  system.time()
```






```{r}
logit_fit |>
  # aggregate metrics from K folds
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = penalty, y = mean, color = factor(mixture))) +
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") +
  scale_x_log10()
```

```{r}
logit_fit |>
  show_best("roc_auc")
```
```{r}
best_logit <- logit_fit |>
  select_best("roc_auc")
best_logit
```

```{r}
# Final workflow
final_wf <- logit_wf |>
  finalize_workflow(best_logit)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf |>
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit |> 
  collect_metrics()
```
```{r}
#feature importance
library(vip)

final_fit %>% 
  extract_fit_parsnip() %>% 
  vip()
```
  
###random forest

```{r}
library(tidymodels)

# Assuming mimic_other is your dataset and los_long is the outcome variable
mimic_other <- mimic_other %>%
  mutate(los_long = as.factor(los_long))  # Convert to factor

# Define the recipe
rf_recipe <- 
  recipe(
    los_long ~ .,
    data = mimic_other
  ) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors())

# Proceed with your model setup and tuning as before

```

```{r}
rf_mod <- 
  rand_forest(
    mode = "classification",
    mtry = tune(),
    trees = tune()
  ) |> 
  set_engine("ranger")
rf_mod
```

```{r}
rf_wf <- workflow() |>
  add_recipe(rf_recipe) |>
  add_model(rf_mod)
rf_wf
```

```{r}
param_grid2 <- grid_regular(
  trees(range = c(100L, 300L)),
  mtry(range = c(1L, 5L)),
  levels = c(5, 5)
  )
param_grid2
```

```{r}
set.seed(203)

folds <- vfold_cv(mimic_other, v = 5)
folds
```

```{r}
rf_fit <- rf_wf |>
  tune_grid(
    resamples = folds,
    grid = param_grid2,
    metrics = metric_set(roc_auc, accuracy)
    )
rf_fit
```


```{r}
rf_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = trees, y = mean, color = factor(mtry))) +
  geom_point() +
  labs(x = "Num. of Trees", y = "CV AUC") 
```

```{r}
rf_fit |>
  show_best("roc_auc")
```
```{r}
best_rf <- rf_fit |>
  select_best("roc_auc")
best_rf
```

```{r}
# Final workflow
final_wf2 <- rf_wf |>
  finalize_workflow(best_rf)
final_wf2
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit2 <- 
  final_wf |>
  last_fit(data_split)
final_fit2
```

```{r}
# Test metrics
final_fit2 |> 
  collect_metrics()
```
```{r}
#feature importance
library(vip)

final_fit2 %>% 
  extract_fit_parsnip() %>% 
  vip()
```

###XGBoost

```{r}
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(xgboost)
gb_recipe <- 
    recipe(
    los_long ~ .,
    data = mimic_other
  ) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors())
gb_recipe

```

```{r}
gb_mod <- 
  boost_tree(
    mode = "classification",
    trees = 1000,
    tree_depth = tune(),
    learn_rate = tune()
  ) |> 
  set_engine("xgboost")
gb_mod
```

```{r}
gb_wf <- workflow() |>
  add_recipe(gb_recipe) |>
  add_model(gb_mod)
gb_wf
```

```{r}
param_grid3 <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3,10)
  )
param_grid3
```

```{r}
set.seed(203)

folds <- vfold_cv(mimic_other, v = 5)
folds
```

```{r}
gb_fit <- gb_wf |>
  tune_grid(
    resamples = folds,
    grid = param_grid3,
    metrics = metric_set(roc_auc, accuracy)
    )
gb_fit
```




```{r}
gb_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```

```{r}
gb_fit |>
  show_best("roc_auc")
```
```{r}
best_gb <- gb_fit |>
  select_best("roc_auc")
best_gb
```

```{r}
# Final workflow
final_wf3 <- gb_wf |>
  finalize_workflow(best_gb)
final_wf3
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit3 <- 
  final_wf3 |>
  last_fit(data_split)
final_fit3
```

```{r}
# Test metrics
final_fit3 |> 
  collect_metrics()
```

```{r}
library(vip)

final_fit3 %>% 
  extract_fit_parsnip() %>% 
  vip()
```
### model stacking approach

```{r}
library(gtsummary)
library(keras)
library(ranger)
library(stacks)
library(tidyverse)
library(tidymodels)
library(xgboost)
mimic_recipe <- 
    recipe(
    los_long ~ .,
    data = mimic_other
  ) |>
  # mean imputation for 
  step_impute_mean(all_numeric_predictors()) |>
  # mode imputation for 
  step_impute_mode(all_nominal_predictors()) |>
  # create traditional dummy variables (necessary for svm)
  step_dummy(all_nominal_predictors()) |>
  # zero-variance filter
  step_zv(all_numeric_predictors()) |> 
  # center and scale numeric data
  step_normalize(all_numeric_predictors()) 
mimic_recipe
```

```{r}
set.seed(203)
folds <- vfold_cv(mimic_other, v = 5)
folds
```
####logit model
```{r}
logit_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = tune()
  ) |> 
  set_engine("glmnet", standardize = FALSE) |>
  print()
```

```{r}
logit_wf <- workflow() |>
  add_recipe(logit_recipe) |>
  add_model(logit_mod) |>
  print()
```

```{r}
param_grid <- grid_regular(
  penalty(range = c(-3, 1)), 
  mixture(),
  levels = c(5, 3)
)

logit_res <- tune_grid(
  object = logit_wf,
  resamples = folds,
  grid = param_grid,
  control = control_stack_grid()
)

logit_res
```
####random forest
```{r}
rf_mod <- 
  rand_forest(
    mode = "classification",
    mtry = tune(),
    trees = tune()
  ) |> 
  set_engine("ranger")
rf_mod
```

```{r}
rf_wf <- workflow() |>
  add_recipe(rf_recipe) |>
  add_model(rf_mod)
rf_wf
```

```{r}
param_grid2 <- grid_regular(
  trees(range = c(100L, 300L)),
  mtry(range = c(1L, 5L)),
  levels = c(5, 5)
  )
param_grid2

rf_res <- tune_grid(
  object = rf_wf,
  resamples = folds,
  grid = param_grid2,
  control = control_stack_grid()
)

rf_res
```

####XGboost

```{r}
gb_mod <- 
  boost_tree(
    mode = "classification",
    trees = 1000,
    tree_depth = tune(),
    learn_rate = tune()
  ) |> 
  set_engine("xgboost")
gb_mod
```

```{r}
gb_wf <- workflow() |>
  add_recipe(gb_recipe) |>
  add_model(gb_mod)
gb_wf
```

```{r}
param_grid3 <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3,10)
  )
param_grid3

gb_res <- 
  tune_grid(
    object = gb_wf, 
    resamples = folds, 
    grid = param_grid3,
    control = control_stack_grid()
  )
gb_res
```


```{r}
mimic_model_st <- 
  # initialize the stack
  stacks() |>
  # add candidate members
  add_candidates(logit_res) |>
  add_candidates(rf_res) |>
  add_candidates(gb_res) |>
  # determine how to combine their predictions
  blend_predictions(
    penalty = 10^(-6:2),
    metrics = c("roc_auc")
    ) |>
  # fit the candidates with nonzero stacking coefficients
  fit_members()
mimic_model_st
```

```{r}
mimic_model_st
```

```{r}
autoplot(mimic_model_st)
```

```{r}
autoplot(mimic_model_st, type = "members")
```

```{r}
autoplot(mimic_model_st, type = "weights")
```

```{r}
collect_parameters(mimic_model_st, "rf_res")
```

```{r}
mimic_pred <- mimic_test %>%
  bind_cols(predict(mimic_model_st, ., type = "prob")) %>%
  print(width = Inf)
```

```{r}
yardstick::roc_auc(
  mimic_pred,
  truth = los_long,
  contains(".pred_FALSE")
  )
```
We can use the members argument to generate predictions from each of the ensemble members.
```{r}
mimic_pred <-
  mimic_test |>
  select(los_long) |>
  bind_cols(
    predict(
      mimic_model_st,
      mimic_test,
      type = "class",
      members = TRUE
      )
    ) |>
  print(width = Inf)
```

```{r}
map(
  colnames(mimic_pred),
  ~mean(mimic_pred$los_long == pull(mimic_pred, .x))
  ) |>
  set_names(colnames(mimic_pred)) |>
  as_tibble() |>
  pivot_longer(c(everything(), -los_long))
```

4. Compare model classification performance on the test set. Report both the area under ROC curve and accuracy for each machine learning algorithm and the model stacking. Interpret the results. What are the most important features in predicting long ICU stays? How do the models compare in terms of performance and interpretability?
```{r}
num_subjects <- mimiciv_icu_cohort %>%
  distinct(subject_id) %>%
  nrow()

print(num_subjects)
```



```{r}
# svm Test metrics
final_fit |> 
  collect_metrics()
```

```{r}
# random forest Test metrics
final_fit2 |> 
  collect_metrics()
```

```{r}
# xgboost Test metrics
final_fit3 |> 
  collect_metrics()
```

```{r}
# stacking Test metrics
mimic_pred <- mimic_test %>%
  bind_cols(predict(mimic_model_st, ., type = "prob")) 

yardstick::roc_auc(
  mimic_pred,
  truth = los_long,
  contains(".pred_FALSE")
  )
```
```{r}
#feature importance
library(vip)

final_fit %>% 
  extract_fit_parsnip() %>% 
  vip()
```

```{r}
#feature importance
library(vip)

final_fit2 %>% 
  extract_fit_parsnip() %>% 
  vip()
```
```{r}
#feature importance
library(vip)

final_fit3 %>% 
  extract_fit_parsnip() %>% 
  vip()
```

In my analysis, there were n = 50920 patients in the MIMIC-IV ICU cohort, and 50/50 split.
I build three machine learning models and a model stacking to predict long ICU stays. The three models are logistic regression, random forest, and xgboost. The model stacking is a combination of the three models. The area under the ROC curve and accuracy for each machine learning algorithm and the model stacking are as follows:
logit: AUC = 0.58, Accuracy = 0.61
Random Forest: AUC = 0.58, Accuracy = 0.61
XGBoost: AUC = 0.66, Accuracy = 0.61
Stacking: AUC = 0.67, Accuracy = 0.63
The most important features in predicting long ICU stays are the temperature fahrenheit, noninvasive blood pressure systolic and first_careunit. 
Interpretation of Results:
Logistic Regression and Random Forest both achieved an AUC of 0.58 and accuracy of 0.61, indicating moderate predictive performance with similar capabilities in distinguishing between short and long ICU stays. The identical scores suggest that, for this task, the complexity of the random forest model does not provide a significant advantage over the simpler logistic regression model in terms of discriminative ability.
XGBoost showed a notable improvement with an AUC of 0.66 while maintaining the same accuracy as the other two models (0.61). This enhancement in AUC signifies better discriminative power, likely due to XGBoost's ability to capture complex, non-linear relationships and interactions between features through gradient boosting.
Stacking further improved the performance, achieving the highest AUC of 0.67 and an accuracy of 0.63. This demonstrates the strength of ensemble learning, where combining the predictions from multiple models leads to a more accurate and robust classifier than any single model alone.
Importance of Features:
Identifying temperature Fahrenheit, noninvasive blood pressure systolic, and first_careunit as the most important features suggests that physiological parameters and initial care unit admission are critical in determining the length of ICU stays. This aligns with clinical intuition, as vital signs reflect a patient's immediate health status, and the type of care unit may indicate the severity of the patient's condition.
Performance vs. Interpretability:
Logistic Regression offers the most straightforward interpretability among the models. The coefficients of the logistic regression model can directly indicate the effect size and direction (positive or negative) of each feature on the likelihood of a long ICU stay.
Random Forest and XGBoost provide insights into feature importance, which helps understand which variables are most predictive of long ICU stays. However, the internal decision-making process (i.e., how the features interact to make predictions) is less transparent than in logistic regression, making these models more challenging to interpret.
Model Stacking achieves the best predictive performance but at the cost of further reduced interpretability. Understanding how individual predictions are combined in the stacking process can be complex, making it more difficult to extract clinically actionable insights.
Conclusion:
The trade-off between performance and interpretability is evident in my analysis. While XGBoost and model stacking show superior performance, they do so at the expense of interpretability. Logistic regression offers a balance between moderate predictive power and ease of interpretation, making it valuable in settings where understanding the influence of specific features is crucial.
For predicting long ICU stays, leveraging the strengths of each model according to the clinical context and decision-making requirements is essential. In practice, combining the predictive power of models like XGBoost with the interpretability of logistic regression, possibly through model explanations or feature importance analyses, could provide a pragmatic approach to improving patient care and resource allocation in the ICU.




